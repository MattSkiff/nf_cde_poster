---
main_topsize: 0.10 #percent coverage of the poster
main_bottomsize: 0.10
#ESSENTIALS
title: " "
primary_colour: "#1a1818" 
secondary_colour: "#9e8a8a"
accent_colour: "#cc0000"
poster_height: "36in"
poster_width: "23.4in"
main_fontfamily: "Helvetica"
main_textsize: "130px"
author_textsize: "1.11em"
body_textsize: "29px"
author:
  - name: "Matthew Skiffington[^1]"
    main: true
    twitter: mattskiff_ 
    email: mks29@students.waikato.ac.nz
affiliation:
    address: Department of Computer Science, University of Waikato
main_findings: 
  - "**Normalising Flows** for **Conditional Density Estimation**"
logoleft_name: "figures/TAIAO_logo_1000x320_upscaled.png"
logoright_name: https&#58;//raw.githubusercontent.com/brentthorne/posterdown/master/images/betterhexlogo.png
logocenter_name: "figures/SVGFullColourHorizontalRGBforredbackground_upscaled.png"
output: 
  posterdown::posterdown_betterport:
    self_contained: false
    fig_caption: yes
    pandoc_args: --mathjax
    number_sections: false
bibliography: bibliography.bibtex
link-citations: true
csl: ieee.csl
---

<!-- logo sizing -->
<style>
.main p {
margin-left: 0em;
}
#main-img-left {
 width: 35.75%;
 height: 72.5%;
 bottom: 0.4in;
}
#code {
 font-size: 30px;
}
#main-img-center {
 width: 36.75%;
 height: 71.5%;
}
#main-img-right {
 bottom: 0.4in;
}
.footnotes {
  font-size: 16pt;
}

</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = 'G:\\My Drive\\PhD\\posters\\')
```

<br>

## Conditional Density Estimation

<!-- add figure caption: Animation of CDE using 'Mexican hat' data --> 

```{r qr,out.extra='style="float:right; padding:10px"',fig.cap="\\label{fig:qr}QR code for CDE animation",out.width="30%",echo=FALSE}
# All defaults
knitr::include_graphics('figures\\qr.png')
```

<!-- equation goes here -->

Conditional density estimation (CDE) is a form of supervised learning with methods in statistics, machine learning and deep learning. It is a generalisation of regression. Instead of predicting a point estimate $\hat{y}$ and generating a confidence or credible interval $\hat{y}\pm CI$, the task is to predict the full conditional density $p(y|x)$ of the data for a given query point $x$, an improved form of **uncertainty quantification (UQ)**. 

Figure \@ref(fig:cde) shows kernel CDE [@bashtannyk2001bandwidth] [@rosenblatt1969conditional] using the faithful geyser data via the `hdrcde` R package, demonstrating an improvement in UQ by using CDE instead of intervals. Early CDE methods included KCDE and discretisation of the target variable via class probability estimators [@frank2009conditional] (Figure \@ref(fig:qr)). Modern methods for CDE include Random Forest-CDE [@pospisil2018rfcde], Mixture Density Networks [@carney2005predicting] and GMMs [@gilardi2002conditional]. 


```{r cde, echo=FALSE, fig.cap="\\label{fig:cde}Demonstration of estimation using KCDE vs Simple Linear Regression & Prediction Intervals", fig.height=12, message=FALSE, warning=FALSE, out.width="90%"}
library(hdrcde)
library(ggplot2)
library(gridExtra)

faithful.cde <- cde(x = faithful$waiting, y = faithful$eruptions,
                    x.name="Waiting time", y.name = "Duration time",
                    x.margin = 80)

faithful2.cde <- cde(x = faithful$waiting, y = faithful$eruptions,
                    x.name="Waiting time", y.name = "Duration time",
                    x.margin = 50)

faithful3.cde <- cde(x = faithful$waiting, y = faithful$eruptions,
                    x.name="Waiting time", y.name = "Duration time",
                    x.margin = 65)

cde_df <- data.frame(y = faithful.cde$y,
                     z = as.vector(faithful.cde$z),
                     z2 = as.vector(faithful2.cde$z),
                     z3 = as.vector(faithful3.cde$z))

p1 <- ggplot(data = cde_df) +
  geom_line(mapping = aes(x = y,y = z),color = 'red') +
  geom_line(mapping = aes(x = y,y = z2),color = 'blue') +
  geom_line(mapping = aes(x = y,y = z3),color = 'purple') +
  theme_light() +
  labs(title = "Conditional Density Estimates (2) of Eruption Duration",
       x = "target",y = "density")

p2 <- ggplot(data = faithful) +
  geom_point(mapping = aes(x = waiting,y = eruptions)) +
  geom_vline(xintercept = 80,color = 'red') + 
  geom_vline(xintercept = 50,color = 'blue') + 
  geom_vline(xintercept = 65,color = 'purple') + 
  theme_light() +
  labs(title = "Scatterplot of Old Faithful Data and query points")


faithful.lm <- lm(eruptions~waiting,data = faithful)
preds <- as.data.frame(predict(faithful.lm, newdata=data.frame(waiting=c(80,50,65)), interval="prediction",
                         level = 0.95))
preds$target <- c(80,50,65)

p3 <- ggplot(data = faithful) +
  geom_point(mapping = aes(x = waiting,y = eruptions)) +
  geom_abline(slope = coef(faithful.lm)[[2]],intercept =  coef(faithful.lm)[[1]],color = 'grey',size = 1) +
  geom_errorbar(data=preds[1,], aes(x = target,ymin = lwr, ymax = upr), fill = "blue", size = 1,color = 'red') +
  geom_errorbar(data=preds[2,], aes(x = target,ymin = lwr, ymax = upr), fill = "blue", size = 1,color = 'blue') +
  geom_errorbar(data=preds[3,], aes(x = target,ymin = lwr, ymax = upr), fill = "purple", size = 1,color = 'purple') +
  theme_light() +
  labs(title = "95% prediction interval for query points")


grid.arrange(p1, p2, p3, nrow = 3,ncol = 1)
```

## Normalising Flows

<!-- key NF equation -->

Normalising flows (NFs) are sequences of invertible, differentiable, transformations (bijections) on a base probability distribution (often a simple Gaussian) to approximate the true density, which may be skewed, multi-modal or complex (even discontinuous) [@kobyzev2020normalizing]. NFs were proposed as a density estimation procedure [@tabak2010density], then for use as approximate posteriors in variational inference (VI) [@rezende2015variational]. They came to prominence for efficiency and expressiveness in both sampling (generative direction) and density evaluation (normalising direction). NFs fit alongside VAEs and GANs as recent DL generative models, however, VAEs and GANs are not efficient for density evaluation. 

> *By the term normalising flows people mean bijections which are convenient to compute, invert, and calculate the determinant of their Jacobian* [@kobyzev2020normalizing].

<!--

>  `r tufte::quote_footer('--- Kobyzev et al, "Normalizing Flows: An Introduction and Review of Current Methods" (2020)')`

-->

```{r nfs, echo=FALSE, fig.cap="\\label{fig:nfs}Expressiveness varies by choice of flow", fig.height=8, message=FALSE, warning=FALSE, out.width="100%",cache=TRUE}
# Simple Flow visualisation

library(MASS)
library(gridExtra)
library(lattice)
library(grid)

n <- 10000

# sampling z from base distribution
mvn.mat <- mvrnorm(n = n,mu = c(0,0),Sigma = diag(2),tol = 1e-6,empirical = TRUE)

mvr_sample.df <- as.data.frame(mvn.mat)

p1 <- ggplot(mvr_sample.df) +
        #geom_point(mapping = aes(x = V1,y = V2),alpha = 0.1) +
        geom_density2d_filled(mapping = aes(x = V1,y = V2)) +
        theme_light() +
        labs(x = "x",y = "y",title = "KDE of Sample from MVN",caption = "n = 100k",subtitle = "Base distribution") +
        scale_x_continuous(limits = c(-5,5), expand = c(0, 0)) +
        scale_y_continuous(limits = c(-5,5), expand = c(0, 0))  + 
        theme(legend.position = "none") 

# diagonal matrix for A
A_diag <- diag(x = runif(n,min = 0,max = 3))
b_diag <- as.integer(runif(1,min = -3, max = 3))

# triangular matrix for A - more expressive
A_tri <- matrix(0, n,n)
# source: https://stackoverflow.com/questions/9282258/how-to-fill-matrix-with-random-numbers-in-r/9282447
A_tri[row(A_tri)+col(A_tri) >=n+1] <- (row(A_tri)+col(A_tri) -n+1)[row(A_tri)+col(A_tri)>=n+1]/n

# random triangular matrix - exp dist
A_tri_exp <- upper.tri(matrix(rexp(n*n, rate=.1), ncol=n))

# random triangular matrix - unif dist\
A_tri_unif <- upper.tri(matrix(runif(n*n,0,3), ncol=n))


linear_transform <- function(x,A,b) {
  return(A %*% x + b)
}

# generative direction
mvn_linear_transform_A_diag.mat <- linear_transform(x = mvn.mat,A = A_diag,b = b_diag)
mvn_linear_transform_A_diag.df <- as.data.frame(mvn_linear_transform_A_diag.mat)

mvn_linear_transform_A_tri_exp.mat <- linear_transform(x = mvn.mat,A = A_tri_exp,b = b_diag)
mvn_linear_transform_A_tri_exp.df <- as.data.frame(mvn_linear_transform_A_tri_exp.mat)

mvn_linear_transform_A_tri_unif.mat <- linear_transform(x = mvn.mat,A = A_tri_exp,b = b_diag)
mvn_linear_transform_A_tri_unif.df <- as.data.frame(mvn_linear_transform_A_tri_unif.mat)

p2 <- ggplot(mvn_linear_transform_A_diag.df) +
        #geom_point(mapping = aes(x = V1,y = V2),alpha = 0.1) +
        geom_density2d_filled(mapping = aes(x = V1,y = V2)) +
        theme_light() +
        labs(x = "x",y = "y",title = expression(A[1]~KDE~of~Linear~transform~on~sample),caption = "n = 100k",subtitle = "Diagonal transformation matrix") +
        scale_x_continuous(limits = c(-5,5), expand = c(0, 0)) +
        scale_y_continuous(limits = c(-5,5), expand = c(0, 0))  + 
        theme(legend.position = "none")

p3 <- ggplot(mvn_linear_transform_A_tri_unif.df) +
  #geom_point(mapping = aes(x = V1,y = V2),alpha = 0.1) +
  geom_density2d_filled(mapping = aes(x = V1,y = V2)) +
  theme_light() +
  labs(x = "x",y = "y",title = expression(A[2]~KDE~of~Linear~transform~on~sample),caption = "n = 100k",subtitle = "Triangular transformation matrix") +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))  + 
  theme(legend.position = "none")

plot_explanation <- "Example of a linear normalising flow\n in the generative direction for linear flows,\n illustrating the difference in expressiveness\n between diagonal and triangular matrices"
t <- textGrob(plot_explanation)
final <- grid.arrange(p1,p2,p3,t,nrow = 2)
```

Normalising flows are trained by maximising log-likelihood via SGD, or minimising KL-divergence when VI is used. Three properties are important for NFs [@kobyzev2020normalizing]:

* Efficiency 
* Expressiveness
* Invertibility 

\usepackage{ dsfont }

The number of layers and class of model both influence the expressiveness of the trained distribution. Figure \@ref(fig:nfs) shows a sample from a MVN: $\mathbf{x} \sim \mathcal{N}(\mathcal{u},\Sigma)$ transformed using two simple linear ($g(x) = \mathbf{A}\mathbf{x} + b$) flows, where $A_1 := diag(v), A_2 := tri(v), v\sim U(0,3)$ and $b \sim U(-3,3), \mathbf{x} \in \mathcal{R^2}$. For $A_1$ and $A_2$, positive entries on the diagonal ensure invertibility.

Practical NF models often use **coupling** functions (NICE, RealNVP, Neural Spline Flows, Glow). Coupling functions split the input into disjoint partions, applying an arbitrarily complex conditioning function (e.g. an invertible NN) to one. Other models use coupling functions where the conditioner is **autoregressive** (MAFs, IAFs, NAFs). Recent developments include continuous NFs using neural ODEs (e.g. FFJORD), work on ordinal data and on manifold learning [@kobyzev2020normalizing].

## NFs for CDE

Normalising flows (NFs) find applications in conditional class probability estimation, conditional image generation and multivariate time series prediction. Work on CDE with NFs is limited. This includes Bayesian NFs, with a framework for priors over CDE estimators using BNNs with VI [@trippe2018conditional]. CDE using MAFs and Real NVP by conditioning each term in the chain rule of probability with the inclusion of $y$ at every layer was proposed in 2017 [@papamakarios2017masked]. This was explored while introducing noise regularisation for CDE in 2019 [@rothfuss2019noise]. Conditional NFs for structured prediction have also been developed [@winkler2019learning]. Progress in using NFs for CDE has been limited by the following factors:

* Computational difficulty of scaling NFs to large data sets - newer continuous NF models are restricted to usage on small image benchmark sets [@grathwohl2018ffjord].
* Deep Learning focus is often in areas of traditional strength, e.g. image data. 

Figure \@ref(fig:nfcdeex) demonstrates using NFs to create CDEs of rainfall & soil moisture (data source: NIWA, 10 months). We scale the data, train the flow, and condition on two levels of rainfall $[-1,1]$ to show the conditional densities. Note the separate estimates have different modality, which could reflect a differing seasonal effect. Other climatological variables and spatio-temporal factors would be extra sources of variation not considered. The marginals are simulated well while the joint is poorly approximated (and noisy). The probabilistic programming libraries `pyro` and `tensorflow probability` both implement NFs.

<br>

```{r nfcdeex,out.extra='style="float:right; padding:10px"',fig.cap="\\label{fig:nfcdeex}Traing NFs for CDE with climatological data",out.width="100%",echo=FALSE}
# All defaults
knitr::include_graphics('figures\\combined.png')
```

## References

[^1]:  Poster produced via the *posterdown* package. The code to reproduce this poster is at **https://github.com/MattSkiff/nf_cde_poster**.