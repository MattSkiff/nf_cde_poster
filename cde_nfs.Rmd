---
main_topsize: 0.15 #percent coverage of the poster
main_bottomsize: 0.1
#ESSENTIALS
title: " "
primary_colour: "#1a1818" 
secondary_colour: "#9e8a8a"
accent_colour: "#cc0000"
poster_height: "36in"
poster_width: "23.4in"
main_fontfamily: "Helvetica"
main_textsize: "130px"
author_textsize: "1.11em"
body_textsize: "35px"
author:
  - name: "Matthew Skiffington"
    main: true
    twitter: mattskiff_ 
    email: mks29@students.waikato.ac.nz
affiliation:
    address: Department of Computer Science, University of Waikato
main_findings: 
  - "**Normalising Flows** for **Conditional Density Estimation**"
logoleft_name: "figures/TAIAO_logo_1000x320_upscaled.png"
logoright_name: https&#58;//raw.githubusercontent.com/brentthorne/posterdown/master/images/betterhexlogo.png
logocenter_name: "figures/SVGFullColourHorizontalRGBforredbackground_upscaled.png"
output: 
  posterdown::posterdown_betterport:
    self_contained: false
    fig_caption: yes
    pandoc_args: --mathjax
    number_sections: false
bibliography: bibliography.bibtex
link-citations: true
csl: ieee.csl
---

<!-- logo sizing -->
<style>
.main p {
margin-left: 0em;
}
#main-img-left {
 width: 35.75%;
 height: 72.5%;
 bottom: 0.4in;
}
#main-img-center {
 width: 36.75%;
 height: 71.5%;
}
#main-img-right {
 bottom: 0.4in;
}
.footnotes {
  font-size: 16pt;
}

</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = 'G:\\My Drive\\PhD\\posters\\')
```

<br>

## Conditional Density Estimation

<!-- add figure caption: Animation of CDE using 'Mexican hat' data --> 

```{r qr,out.extra='style="float:right; padding:10px"',fig.cap="\\label{fig:qr}QR code for CDE animation",out.width="30%",echo=FALSE}
# All defaults
knitr::include_graphics('figures\\qr.png')
```

<!-- equation goes here -->

Conditional density estimation is a form of supervised learning for which there are methods in statistics, machine learning and deep learning. It is a generalisation of regression. In CDE, instead of predicting a point estimate $\hat{y}$ and generated a confidence or credible interval $\hat{y}\pm CI$, the task is to predict the full conditional density $p(y|x)$ of the data for a given query point $x$, representing an improved form of **uncertainty quantification**. 

Figure \@ref(fig:cde) demonstrates kernel CDE [@bashtannyk2001bandwidth] [@rosenblatt1969conditional] using the old faithful greyser dataset via the `hdrcde` R package, showing an improvement in how informative uncertainty estimates are using CDE instead of prediction intervals. Early CDE methods included KCDE and discretisation of the target variable via class probability estimators [@frank2009conditional] (Figure \@ref(fig:qr)). Modern methods for CDE include Random Forest-CDE [@pospisil2018rfcde], Mixture Density Networks [@carney2005predicting] and GMMs [@gilardi2002conditional]. 


```{r cde, echo=FALSE, fig.cap="\\label{fig:cde}Demonstration of estimation using KCDE vs SLR & PIs", fig.height=11, message=FALSE, warning=FALSE, out.width="90%"}
library(hdrcde)
library(ggplot2)
library(gridExtra)

faithful.cde <- cde(x = faithful$waiting, y = faithful$eruptions,
                    x.name="Waiting time", y.name = "Duration time",
                    x.margin = 80)

faithful2.cde <- cde(x = faithful$waiting, y = faithful$eruptions,
                    x.name="Waiting time", y.name = "Duration time",
                    x.margin = 50)

faithful3.cde <- cde(x = faithful$waiting, y = faithful$eruptions,
                    x.name="Waiting time", y.name = "Duration time",
                    x.margin = 65)

cde_df <- data.frame(y = faithful.cde$y,
                     z = as.vector(faithful.cde$z),
                     z2 = as.vector(faithful2.cde$z),
                     z3 = as.vector(faithful3.cde$z))

p1 <- ggplot(data = cde_df) +
  geom_line(mapping = aes(x = y,y = z),color = 'red') +
  geom_line(mapping = aes(x = y,y = z2),color = 'blue') +
  geom_line(mapping = aes(x = y,y = z3),color = 'purple') +
  theme_light() +
  labs(title = "Conditional Density Estimates (2) of Eruption Duration",
       x = "target",y = "density")

p2 <- ggplot(data = faithful) +
  geom_point(mapping = aes(x = waiting,y = eruptions)) +
  geom_vline(xintercept = 80,color = 'red') + 
  geom_vline(xintercept = 50,color = 'blue') + 
  geom_vline(xintercept = 65,color = 'purple') + 
  theme_light() +
  labs(title = "Scatterplot of Old Faithful Data and CDE query points")


faithful.lm <- lm(eruptions~waiting,data = faithful)
preds <- as.data.frame(predict(faithful.lm, newdata=data.frame(waiting=c(80,50,65)), interval="prediction",
                         level = 0.95))
preds$target <- c(80,50,65)

p3 <- ggplot(data = faithful) +
  geom_point(mapping = aes(x = waiting,y = eruptions)) +
  geom_abline(slope = coef(faithful.lm)[[2]],intercept =  coef(faithful.lm)[[1]],color = 'grey',size = 1) +
  geom_errorbar(data=preds[1,], aes(x = target,ymin = lwr, ymax = upr), fill = "blue", size = 1,color = 'red') +
  geom_errorbar(data=preds[2,], aes(x = target,ymin = lwr, ymax = upr), fill = "blue", size = 1,color = 'blue') +
  geom_errorbar(data=preds[3,], aes(x = target,ymin = lwr, ymax = upr), fill = "purple", size = 1,color = 'purple') +
  theme_light() +
  labs(title = "Scatterplot of Old Faithful Data and SLR",subtitle = "95% prediction interval for query points")


grid.arrange(p1, p2, p3, nrow = 3,ncol = 1)
```

## Normalising Flows

<!-- key NF equation -->

Normalising flows are a *class of models* [@kobyzev2020normalizing] that are sequences of invertible, differentiable, transformations (bijections) upon a base probability distribution (typically a simple gaussian) to approximate the true data density, which may be skewed, multi-modal or complex (even discontinuous). NFs came to prominence for their relative efficiency in both generative sampling and density evaluation (the normalising direction). In generative modeling, NFs fit alongside VAEs and GANs. However, VAEs and GANs are not efficient for density evaluation.

> *By the term normalising flows people mean bijections which are convenient to compute, invert, and calculate the determinant of their Jacobian*.

>  `r tufte::quote_footer('--- Kobyzev et al, "Normalizing Flows: An Introduction and Review of Current Methods" (2020)')`

```{r nfs, echo=FALSE, fig.cap="\\label{fig:nfs}Expressiveness of NFs varies by class of model", fig.height=8, message=FALSE, warning=FALSE, out.width="100%",cache=TRUE}
# Simple Flow visualisation

library(MASS)
library(gridExtra)
library(lattice)
library(grid)

n <- 1000

# sampling z from base distribution
mvn.mat <- mvrnorm(n = n,mu = c(0,0),Sigma = diag(2),tol = 1e-6,empirical = TRUE)

mvr_sample.df <- as.data.frame(mvn.mat)

p1 <- ggplot(mvr_sample.df) +
        geom_point(mapping = aes(x = V1,y = V2),alpha = 0.1) +
        theme_light() +
        labs(x = "x",y = "y",title = "Sample from MVN",caption = "n = 1k",subtitle = "Base distribution") +
        scale_x_continuous(limits = c(-5,5), expand = c(0, 0)) +
        scale_y_continuous(limits = c(-5,5), expand = c(0, 0))  + 
        theme(legend.position = "none") 

# diagonal matrix for A
A_diag <- diag(x = runif(n,min = 0,max = 3))
b_diag <- as.integer(runif(1,min = -3, max = 3))

# triangular matrix for A - more expressive
A_tri <- matrix(0, n,n)
# source: https://stackoverflow.com/questions/9282258/how-to-fill-matrix-with-random-numbers-in-r/9282447
A_tri[row(A_tri)+col(A_tri) >=n+1] <- (row(A_tri)+col(A_tri) -n+1)[row(A_tri)+col(A_tri)>=n+1]/n

# random triangular matrix - exp dist
A_tri_exp <- upper.tri(matrix(rexp(n*n, rate=.1), ncol=n))

# random triangular matrix - unif dist\
A_tri_unif <- upper.tri(matrix(runif(n*n,0,3), ncol=n))


linear_transform <- function(x,A,b) {
  return(A %*% x + b)
}

# generative direction
mvn_linear_transform_A_diag.mat <- linear_transform(x = mvn.mat,A = A_diag,b = b_diag)
mvn_linear_transform_A_diag.df <- as.data.frame(mvn_linear_transform_A_diag.mat)

mvn_linear_transform_A_tri_exp.mat <- linear_transform(x = mvn.mat,A = A_tri_exp,b = b_diag)
mvn_linear_transform_A_tri_exp.df <- as.data.frame(mvn_linear_transform_A_tri_exp.mat)

mvn_linear_transform_A_tri_unif.mat <- linear_transform(x = mvn.mat,A = A_tri_exp,b = b_diag)
mvn_linear_transform_A_tri_unif.df <- as.data.frame(mvn_linear_transform_A_tri_unif.mat)

p2 <- ggplot(mvn_linear_transform_A_diag.df) +
        geom_point(mapping = aes(x = V1,y = V2),alpha = 0.1) +
        theme_light() +
        labs(x = "x",y = "y",title = expression(A[1]~Linear~transform~on~sample),caption = "n = 1k",subtitle = "Diagonal transformation matrix") +
        scale_x_continuous(expand = c(0, 0)) +
        scale_y_continuous(expand = c(0, 0))  + 
        theme(legend.position = "none")

p3 <- ggplot(mvn_linear_transform_A_tri_unif.df) +
  geom_point(mapping = aes(x = V1,y = V2),alpha = 0.1) +
  theme_light() +
  labs(x = "x",y = "y",title = expression(A[2]~Linear~transform~on~sample),caption = "n = 1k",subtitle = "Triangular transformation matrix") +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))  + 
  theme(legend.position = "none")

plot_explanation <- "Example of a linear normalising flow\n in the generative direction for linear flows,\n illustrating the difference in expressiveness\n between diagonal and triangular matrices"
t <- textGrob(plot_explanation)
final <- grid.arrange(p1,p2,p3,t,nrow = 2)
```

Normalising flows are trained by maximising the log-likelihood, or minimising KL-divergence when variational inference is used. There are three properties of concern when designing NFs [@kobyzev2020normalizing]:

* Efficiency
* Expressiveness
* Invertibility 

The number of layers influences the expressiveness of the trained distribution, as does the class of NF. Figure \@ref(fig:nfs) shows a sample from a MVN: $\mathbf{x} \sim \mathcal{N}(\mathcal{u},\Sigma)$ transformed using two simple linear ($g(x) = \mathbf{A}\mathbf{x} + b$) flows , where $A_1 := diag(v), A_2 := tri(v), v\sim U(0,3)$. For $A_1$ and $A_2$, positive entries on the diagonal ensure invertibility.

Practical NFs models often use **coupling** functions (NICE, RealNVP, Neural Spline Flows, Glow). Coupling functions split the input into disjoint partions, applying an arbitrarily complex conditioning function (e.g. an invertible NN) to one partition. Other models use coupling functions where the conditioner is **autoregressive** (MAFs, IAFs, NAFs). Recent developments include continuous NFs using neural ODEs (FFJORD) and research on manifold learning [@kobyzev2020normalizing].

## NFs for CDE

Existing work includes CDE with Bayesian NFs [@trippe2018conditional] and CDE using MAFs [@papamakarios2017masked] but progress has been limited by the following factors:

* Computational difficulty of scaling NFs to large data sets. For example, the latest continuous normalising flows models are restricted to usage on comparatively small image benchmark sets [@grathwohl2018ffjord].
* Few practical applications of CDE in mainstream environmental science.
* DL focus is often in areas of traditional strength for the field, such as image data. 

## References

[^1]:  Poster produced via the *posterdown* package. The code to reproduce this poster is at **https://github.com/MattSkiff/nf_cde_poster**.